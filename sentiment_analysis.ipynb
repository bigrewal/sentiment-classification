{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from utils import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: (7086,)\n",
      "Sanity Check:  we're gonna like watch Mission Impossible or Hoot.(\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./data/data.txt\"\n",
    "data_X, data_Y, m = preprocess(filepath)\n",
    "\n",
    "print(\"Total number of examples:\",data_X.shape)\n",
    "print(\"Sanity Check: \",data_X[0] , data_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input data\n",
    "<p> Prepare Training Data, Validation set and test set. Here we will use 60-20-20 split. We will use the <b>split_dataset(data_X, data_Y)</b> method defined in utils.py </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Size: (4251,) (4251,)\n",
      "Dev Set Size: (1417,) (1417,)\n",
      "Test Set Size: (1418,) (1418,)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, dev_X, dev_Y, test_X, test_Y = split_dataset(data_X, data_Y)\n",
    "\n",
    "print(\"Train Set Size:\", train_X.shape, train_Y.shape)\n",
    "print(\"Dev Set Size:\", dev_X.shape, dev_Y.shape)\n",
    "print(\"Test Set Size:\", test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "\n",
    "### Tokenise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 162 ms, sys: 2.24 ms, total: 164 ms\n",
      "Wall time: 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_words=10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "tokenizer.fit_on_texts(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text to sequence of tokens\n",
    "x_train_tokens = tokenizer.texts_to_sequences(train_X)\n",
    "x_dev_tokens = tokenizer.texts_to_sequences(dev_X)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9294383290996331\n",
      "(4251, 25)\n",
      "Non-padded tokenized sequence:  [  1 108   2 102  16  17]\n",
      "Padded tokenized sequence:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   1 108   2 102  16  17]\n"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_dev_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "\n",
    "print(np.sum(num_tokens < max_tokens) / len(num_tokens))\n",
    "\n",
    "pad = 'pre'\n",
    "\n",
    "#Pad training set\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "#Pad dev data\n",
    "x_dev_pad = pad_sequences(x_dev_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "#Pad test data\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "\n",
    "print(x_train_pad.shape)\n",
    "print(\"Non-padded tokenized sequence: \",np.array(x_train_tokens[1]))\n",
    "print(\"Padded tokenized sequence: \",np.array(x_train_pad[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sentiment_analysis(num_words, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 25, 8)             80000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 16)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 8)           800       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 4)                 208       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 82,613\n",
      "Trainable params: 82,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train the model </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4251/4251 [==============================]4251/4251 [==============================] - 9s 2ms/step - loss: 0.6871 - acc: 0.5483\n",
      "\n",
      "Epoch 2/5\n",
      "4251/4251 [==============================]4251/4251 [==============================] - 5s 1ms/step - loss: 0.6629 - acc: 0.5876\n",
      "\n",
      "Epoch 3/5\n",
      "4251/4251 [==============================]4251/4251 [==============================] - 5s 1ms/step - loss: 0.4821 - acc: 0.8386\n",
      "\n",
      "Epoch 4/5\n",
      "4251/4251 [==============================]4251/4251 [==============================] - 5s 1ms/step - loss: 0.2690 - acc: 0.9330\n",
      "\n",
      "Epoch 5/5\n",
      "4251/4251 [==============================]4251/4251 [==============================] - 5s 1ms/step - loss: 0.1906 - acc: 0.9490\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x123acd3c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad, train_Y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model on test and development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1417/1417 [==============================]1417/1417 [==============================] - 1s 583us/step\n",
      "\n",
      "Accuracy: 93.37%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_dev_pad, dev_Y)\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1418/1418 [==============================]1418/1418 [==============================] - 1s 431us/step\n",
      "\n",
      "Accuracy: 94.50%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test_pad, test_Y)\n",
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25)\n",
      "[[0.9073302]]\n"
     ]
    }
   ],
   "source": [
    "#Use the model on new Examples\n",
    "\n",
    "text = [\"I loved the film\"]\n",
    "tokens = tokenizer.texts_to_sequences(text)\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "print(tokens_pad.shape)\n",
    "result = model.predict(tokens_pad)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
